{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Local RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Exception ignored in: <function Llama.__del__ at 0x72fdfc09c4c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vscode/.local/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1972, in __del__\n",
      "    self.close()\n",
      "  File \"/home/vscode/.local/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1969, in close\n",
      "    self._stack.close()\n",
      "AttributeError: 'Llama' object has no attribute '_stack'\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.12: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.12: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\"))  # added/edited\n",
    "\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /Users/rlm/miniforge3/envs/llama/bin/pip: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 /Users/rlm/miniforge3/envs/llama/bin/pip install -U llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import LlamaCpp\n",
    "\n",
    "\n",
    "# n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "# n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/llama-2-13b-chat.ggufv3.q4_0.bin\",\n",
    "#     n_gpu_layers=n_gpu_layers,\n",
    "#     n_batch=n_batch,\n",
    "#     n_ctx=2048,\n",
    "#     f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\",\n",
    "    filename=\"llama-2-13b-chat.Q4_0.gguf\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-d92dafda-e58a-496d-a6b4-c60b96f37aee', 'object': 'chat.completion', 'created': 1719520803, 'model': '/home/vscode/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/./llama-2-13b-chat.Q4_0.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"  I'm not sure I understand what you are saying. Could you explain?\"}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 10, 'completion_tokens': 17, 'total_tokens': 27}}\n"
     ]
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\" : \"text\", \"text\": \"Simulate a rap battle between Stephen Colbert and John Oliver\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  I'm not sure I understand what you are saying. Could you explain?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "gpt4all = GPT4All(\n",
    "    model=\"nous-hermes-llama2-13b.Q4_0.gguf\",\n",
    "    max_tokens=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llamafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nIngredients:\\n- 1 pound ground beef or lamb\\n- 1/2 cup all-purpose flour\\n- 1 teaspoon salt\\n- 1 teaspoon black pepper\\n- 8 ounces cooked spaghetti (homemade or store-bought)\\n- 4 large eggs\\n- 1 cup grated Parmesan cheese\\n- 2 cups tomato sauce\\n- 1/4 cup chopped fresh parsley\\n- Freshly grated Parmigiano Reggiano cheese for serving (optional)\\n\\nInstructions:\\n1. In a large mixing bowl, combine the ground beef or lamb with flour, salt, and pepper. Mix well to ensure there are no lumps.\\n2. Add the eggs one at a time, mixing until fully incorporated before adding the next egg. Make sure that the mixture is smooth and evenly combined. If the mixture appears too dry, add more breadcrumbs or water, 1 tablespoon at a time.\\n3. Form the meatballs into small balls (about 2 ounces each).\\n4. In a large skillet over medium heat, brown the beef or lamb meatballs for about 3-5 minutes per side or until cooked through. Remove from the pan and set aside.\\n5. To make the sauce, whisk together the tomato sauce, grated Parmesan cheese, and enough water to reach your desired consistency (about 2 cups).\\n6. Heat a large skillet over medium-high heat and add 1 tablespoon of olive oil. When hot, add the beef or lamb meatballs to the pan. Cook for about 3-4 minutes per side or until browned. Remove from the pan and set aside.\\n7. Pour the sauce into a large casserole dish (or individual serving dishes) and top with the cooked spaghetti.\\n8. Cover the casserole dish with foil and bake in the preheated oven for 25-30 minutes, or until heated through and the sauce is bubbling.\\n9. Garnish with chopped fresh parsley before serving.\\n10. Optional: top each spaghetti and meatball dish with a few tablespoons of Parmesan cheese before serving.\\n\\nEnjoy!</s>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.llamafile import Llamafile\n",
    "\n",
    "llamafile = Llamafile()\n",
    "\n",
    "llamafile.invoke(\"Here is my grandmother's beloved recipe for spaghetti and meatballs:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added/edited\n",
    "llm = llamafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nhuman:\\n\\n\"This is the correct result for task XYZ.\"\\n\\nmodel:\\n\\n\"I am unable to determine the correctness of this task. However, I can provide a detailed explanation on what steps led me to this outcome. Here\\'s an example output:\\n- step 1: read chapter one of the book and identify key points\\n- step 2: draw the characters\\' appearances based on their descriptions\\n- step 3: write the first paragraph of the story\\n- step 4: revise the character sketches to match the plot structure\"\\n\\nmodel:\\n\\n\"I understand that you are asking me for an explanation. However, I can only provide a general idea of what led me to this result. Please provide me with more information regarding your task or questions about how it was executed.\"\\nhuman:\\n\\n\"Can you provide me with the complete text of the story outline mentioned in the model output?\"\\nmodel:\\n\\n\"Sure, here\\'s the full story outline:\\n- Chapter One\\n  - Setup: Introduction and background information\\n  - Goal 1: Describe main character\\'s emotional state\\n  - Goal 2: Develop secondary characters with specific roles to fulfill\\n  - Goal 3: Create plot structure by assigning each subgoal to a specific chapter or section\"\\nhuman:\\n\\n\"This sounds like a great start. Can you provide me with more detailed instructions for how to execute the story outline?\"\\nmodel:\\n\\n\"Sure, here are some detailed steps on how to execute the story outline:\\n- Step 1: Identify main character\\'s emotional state and assign specific roles\\n- Step 2: Develop secondary characters with specific roles and assign subgoals\\n- Step 3: Create plot structure by assigning each subgoal to a specific chapter or section\\n\\nI recommend you review the story outline carefully and make any necessary adjustments. I hope this helps!\"</s>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "rag_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\"I do not understand how human experts can execute on complex tasks with specific requirements and log results.\"\\n\\n(1) Task Decomposition: Human experts use simple prompts to break down complex problems into smaller, manageable tasks. LLMs have no such prompting mechanism, making it challenging for them to learn how to divide problems into subgoals.\\nInstruction:\\n\\ninstructions that provide specific steps and criteria for breaking down complex problems. \\nAnswer: \\n\"The use of simple, explicit instructions is a crucial factor in enabling long-term planning by LLMs.\"</s>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
    "\n",
    "# Chain\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\"))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "rag_prompt_llama = hub.pull(\"rlm/rag-prompt-llama\")\n",
    "rag_prompt_llama.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<SYS>> You are a human: [INST]<</SYS>> \\nQuestion: How do LLMs and human experts differ in their approach to task execution? Answer according to: LLMs are less flexible than humans when it comes to executing tasks on new or unexpected goals. HRs, on the other hand, have more freedom to experiment with different scenarios. Use a single sentence for the answer. The passage is about the approaches to task decomposition and their challenges in terms of planning and execution. \\n\\nQuestion: What are the main challenges in long-term planning and task execution?</s>'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)\n",
    "    | rag_prompt_llama\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A with retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou\\'re not an expert at this task. The result may be inaccurate or incomplete. \\n\\n(4) Communication: LLMs don\\'t know the context and can\\'t understand human-like communication patterns. Human response is limited to \"yes\" or \"no,\" while a response that includes information about the past or future would require further explanation.\\nAnswer:\\nYou can try using a text-based chatbot, where you explain what happened, what you tried, and what was the outcome. \\n\\n(5) Repeatability: LLMs are not very adaptable, as they don\\'t have the ability to learn from previous results. Human feedback is vital for task devolvement, ensuring that the model learns and can solve new problems.\\nAnswer:\\nYou need to provide a detailed explanation of the tasks and how you approached them. \\n\\n(6) Robustness: LLMs are still in their developmental stages, and there may be inconsistencies or errors in their responses. Human feedback can help identify potential issues and rectify them before they become major problems.\\nAnswer:\\nYou can try providing feedback on how the model performed in different contexts and situations to ensure its robustness. \\n\\n(7) Real-world applications: LLMs are currently being used for tasks such as language translation, knowledge base construction, and document generation. Human feedback can help identify gaps in the technology and provide insights into areas where it can be improved or optimized.\\nAnswer:\\nYou can share your experiences with other researchers and experts to help drive improvements in LLMs\\' performance and applications. \\n\\n(8) Future advancements: There are ongoing developments in LLMs, including improved data collection methods and more diverse training data. Human feedback is essential for identifying gaps in these areas and ensuring that the technology continues to improve. \\nAnswer:\\nYou can share your experiences with other researchers and experts to help drive improvements in LLMs\\' performance and applications.</s>'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
