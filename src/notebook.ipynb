{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple LLM Application with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "# 0. Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "# 2. Create model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# 5. App definition\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 6. Adding chain route\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Your| name| is| Bob|.||"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the gpt-3.5-turbo model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize a dictionary to store session histories\n",
    "store = {}\n",
    "\n",
    "\n",
    "# Function to retrieve or create a session history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Define a chat prompt template with system and placeholder messages\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a trimmer to trim messages to a maximum token count\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# Define the initial set of messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "]\n",
    "\n",
    "# Create a runnable chain that processes messages\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# Create a runnable with message history, binding the chain with the session history function\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "# Configuration dictionary for the session\n",
    "config = {\"configurable\": {\"session_id\": \"abc16\"}}\n",
    "\n",
    "# Stream responses by passing messages and language configuration to the runnable with message history\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vector stores and retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats are independent pets that often enjoy their own space.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create a list of documents, each with content and metadata\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create a Chroma vector store from the documents, using OpenAI embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},  # Retrieve the top 1 similar document\n",
    ")\n",
    "\n",
    "# Initialize the OpenAI model with the specified version\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Define a message template for the chat prompt\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Create a chat prompt template from the message\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "# Define the RAG (Retrieval-Augmented Generation) chain with context retriever and question passthrough\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "# Invoke the RAG chain with a question about cats\n",
    "response = rag_chain.invoke(\"tell me about cats\")\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hello Bob! How can I assist you today regarding San Francisco?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 90, 'total_tokens': 104}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8848f240-6fc6-47d3-b97c-686f65677ed8-0', usage_metadata={'input_tokens': 90, 'output_tokens': 14, 'total_tokens': 104})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_fIDiygPZgTJJpv0X6C6VcSlJ', 'function': {'arguments': '{\"query\":\"weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 119, 'total_tokens': 140}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-51431379-a950-4d1b-a996-cc44154543f4-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_fIDiygPZgTJJpv0X6C6VcSlJ'}], usage_metadata={'input_tokens': 119, 'output_tokens': 21, 'total_tokens': 140})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1719430643, \\'localtime\\': \\'2024-06-26 12:37\\'}, \\'current\\': {\\'last_updated_epoch\\': 1719430200, \\'last_updated\\': \\'2024-06-26 12:30\\', \\'temp_c\\': 17.2, \\'temp_f\\': 63.0, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 10.5, \\'wind_kph\\': 16.9, \\'wind_degree\\': 260, \\'wind_dir\\': \\'W\\', \\'pressure_mb\\': 1018.0, \\'pressure_in\\': 30.05, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 70, \\'cloud\\': 25, \\'feelslike_c\\': 17.2, \\'feelslike_f\\': 63.0, \\'windchill_c\\': 15.7, \\'windchill_f\\': 60.3, \\'heatindex_c\\': 15.8, \\'heatindex_f\\': 60.5, \\'dewpoint_c\\': 10.9, \\'dewpoint_f\\': 51.7, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 5.0, \\'gust_mph\\': 13.8, \\'gust_kph\\': 22.2}}\"}, {\"url\": \"https://world-weather.info/forecast/usa/san_francisco/june-2024/\", \"content\": \"Extended weather forecast in San Francisco. Hourly Week 10 days 14 days 30 days Year. Detailed \\\\u26a1 San Francisco Weather Forecast for June 2024 - day/night \\\\ud83c\\\\udf21\\\\ufe0f temperatures, precipitations - World-Weather.info.\"}]', name='tavily_search_results_json', tool_call_id='call_fIDiygPZgTJJpv0X6C6VcSlJ')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content=\"The current weather in San Francisco is partly cloudy with a temperature of 63.0°F (17.2°C). The wind is blowing from the west at 16.9 km/h. If you'd like more detailed information or an extended forecast, you can visit [this link](https://www.weatherapi.com/).\", response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 652, 'total_tokens': 719}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0482994e-6006-447e-82ba-31415d3cd8b5-0', usage_metadata={'input_tokens': 652, 'output_tokens': 67, 'total_tokens': 719})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Initialize the search tool with a limit of 2 results per query\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "# Initialize an in-memory SQLite database for saving agent state\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Create a reactive agent executor with the model, tools, and memory\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "# Configuration for the agent's execution, including a thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# Execute the agent with a greeting message and print the response chunks\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"hi im bob! and i live in sf\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "\n",
    "# Execute the agent with a conversational memory query and print the response chunks\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather where I live?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
