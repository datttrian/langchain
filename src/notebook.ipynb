{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify text into labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment='happy' aggressiveness=1 language='spanish'\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Define a Pydantic model for classification\n",
    "class Classification(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"Describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the OpenAI model with specified temperature and model, and configure it to use structured output\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\").with_structured_output(\n",
    "    Classification\n",
    ")\n",
    "\n",
    "# Define a prompt template for extracting desired information\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Extract the desired information from the following passage.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create a chain that combines the prompt and the LLM\n",
    "chain = tagging_prompt | llm\n",
    "\n",
    "# Define the input text for classification\n",
    "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
    "\n",
    "# Invoke the chain with the input text and print the result\n",
    "print(chain.invoke({\"input\": inp}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Extraction Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with temperature set to 0 for deterministic output\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # The 'human' part of the prompt will be populated with the input text\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define a Pydantic model for extracting information about a person\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # The name of the person\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    # The color of the person's hair if known\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the person's hair if known\"\n",
    "    )\n",
    "    # Height measured in meters\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define a Pydantic model for extracting data about multiple people\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # A list of Person objects\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Create a runnable pipeline that combines the prompt and the LLM with structured output\n",
    "runnable = prompt | llm.with_structured_output(schema=Data)\n",
    "\n",
    "# Define the input text for extraction\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "\n",
    "# Invoke the pipeline with the input text and print the result\n",
    "result = runnable.invoke({\"text\": text})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a PDF ingestion and Question/Answering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': \"What was Nike's revenue in 2023?\", 'context': [Document(page_content='Table of Contents\\nFISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS\\nThe following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:\\nFISCAL 2023 COMPARED TO FISCAL 2022\\n•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.\\nThe increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6,\\n2 and 1 percentage points to NIKE, Inc. Revenues, respectively.\\n•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This\\nincrease was primarily due to higher revenues in Men\\'s, the Jordan Brand, Women\\'s and Kids\\' which grew 17%, 35%,11% and 10%, respectively, on a wholesale\\nequivalent basis.', metadata={'page': 35, 'source': 'nke-10k-2023.pdf'}), Document(page_content='Enterprise Resource Planning Platform, data and analytics, demand sensing, insight gathering, and other areas to create an end-to-end technology foundation, which we\\nbelieve will further accelerate our digital transformation. We believe this unified approach will accelerate growth and unlock more efficiency for our business, while driving\\nspeed and responsiveness as we serve consumers globally.\\nFINANCIAL HIGHLIGHTS\\n•In fiscal 2023, NIKE, Inc. achieved record Revenues of $51.2 billion, which increased 10% and 16% on a reported and currency-neutral basis, respectively\\n•NIKE Direct revenues grew 14% from $18.7 billion in fiscal 2022 to $21.3 billion in fiscal 2023, and represented approximately 44% of total NIKE Brand revenues for\\nfiscal 2023\\n•Gross margin for the fiscal year decreased 250 basis points to 43.5% primarily driven by higher product costs, higher markdowns and unfavorable changes in foreign\\ncurrency exchange rates, partially offset by strategic pricing actions', metadata={'page': 30, 'source': 'nke-10k-2023.pdf'}), Document(page_content=\"Table of Contents\\nNORTH AMERICA\\n(Dollars in millions) FISCAL 2023FISCAL 2022 % CHANGE% CHANGE\\nEXCLUDING\\nCURRENCY\\nCHANGESFISCAL 2021 % CHANGE% CHANGE\\nEXCLUDING\\nCURRENCY\\nCHANGES\\nRevenues by:\\nFootwear $ 14,897 $ 12,228 22 % 22 %$ 11,644 5 % 5 %\\nApparel 5,947 5,492 8 % 9 % 5,028 9 % 9 %\\nEquipment 764 633 21 % 21 % 507 25 % 25 %\\nTOTAL REVENUES $ 21,608 $ 18,353 18 % 18 %$ 17,179 7 % 7 %\\nRevenues by:    \\nSales to Wholesale Customers $ 11,273 $ 9,621 17 % 18 %$ 10,186 -6 % -6 %\\nSales through NIKE Direct 10,335 8,732 18 % 18 % 6,993 25 % 25 %\\nTOTAL REVENUES $ 21,608 $ 18,353 18 % 18 %$ 17,179 7 % 7 %\\nEARNINGS BEFORE INTEREST AND TAXES $ 5,454 $ 5,114 7 % $ 5,089 0 %\\nFISCAL 2023 COMPARED TO FISCAL 2022\\n•North America revenues increased 18% on a currency-neutral basis, primarily due to higher revenues in Men's and the Jordan Brand. NIKE Direct revenues\\nincreased 18%, driven by strong digital sales growth of 23%, comparable store sales growth of 9% and the addition of new stores.\", metadata={'page': 39, 'source': 'nke-10k-2023.pdf'}), Document(page_content=\"Table of Contents\\nEUROPE, MIDDLE EAST & AFRICA\\n(Dollars in millions) FISCAL 2023FISCAL 2022 % CHANGE% CHANGE\\nEXCLUDING\\nCURRENCY\\nCHANGESFISCAL 2021 % CHANGE% CHANGE\\nEXCLUDING\\nCURRENCY\\nCHANGES\\nRevenues by:\\nFootwear $ 8,260 $ 7,388 12 % 25 %$ 6,970 6 % 9 %\\nApparel 4,566 4,527 1 % 14 % 3,996 13 % 16 %\\nEquipment 592 564 5 % 18 % 490 15 % 17 %\\nTOTAL REVENUES $ 13,418 $ 12,479 8 % 21 %$ 11,456 9 % 12 %\\nRevenues by:    \\nSales to Wholesale Customers $ 8,522 $ 8,377 2 % 15 %$ 7,812 7 % 10 %\\nSales through NIKE Direct 4,896 4,102 19 % 33 % 3,644 13 % 15 %\\nTOTAL REVENUES $ 13,418 $ 12,479 8 % 21 %$ 11,456 9 % 12 %\\nEARNINGS BEFORE INTEREST AND TAXES $ 3,531 $ 3,293 7 % $ 2,435 35 % \\nFISCAL 2023 COMPARED TO FISCAL 2022\\n•EMEA revenues increased 21% on a currency-neutral basis, due to higher revenues in Men's, the Jordan Brand, Women's and Kids'. NIKE Direct revenues\\nincreased 33%, driven primarily by strong digital sales growth of 43% and comparable store sales growth of 22%.\", metadata={'page': 40, 'source': 'nke-10k-2023.pdf'})], 'answer': \"Nike's revenue in fiscal 2023 was $51.2 billion.\"}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the specified version\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Define the path to the PDF file\n",
    "file_path = \"nke-10k-2023.pdf\"\n",
    "\n",
    "# Load the documents from the PDF file\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the loaded documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a Chroma vector store from the document chunks, using OpenAI embeddings\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define a system prompt for answering questions\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a chat prompt template for answering questions\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine retrieved documents and the question-answering task\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create a retrieval-augmented generation (RAG) chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Invoke the RAG chain with a question and retrieve the results\n",
    "results = rag_chain.invoke({\"input\": \"What was Nike's revenue in 2023?\"})\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a local RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Determine the task: Task decomposition is used when a given task has multiple subgoals or goals. In this case, it's best to identify the most important goal or goals and then separate them into their respective tasks. For example, \"Write a story outline\" might be a separate task from \"Write a novel.\"\n",
      "\n",
      "Step 2: Develop prompts for each task: Determine how you will guide the assistant towards achieving each subgoal or goal. Depending on the context, this can involve asking questions, providing instructions, or suggesting solutions. For example, if you're working on writing a novel and your assistant wants to know what the subgoals are, you might give them \"Write a story outline.\"\n",
      "\n",
      "Step 3: Provide feedback and guidance: As the assistant works through each task, provide feedback and guidance as necessary to help them succeed. For example, if they're working on writing a story outline, you may ask them questions about their approach or suggest improvements based on your prior experience with this type of work.\n",
      "\n",
      "Step 4: Celebrate successes: When the assistant completes each task successfully, celebrate their achievement! This can help reinforce and build confidence in their abilities.\n",
      "\n",
      "Overall, using task decomposition is a great way to break down complex tasks into smaller, more manageable parts. By providing prompts, feedback, and guidance as needed, you can help your assistant succeed and achieve their goals.</s>\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.llms.llamafile import Llamafile\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the data from a web page\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split the loaded data into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Create a Chroma vector store from the document chunks, using HuggingFace embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Initialize the Llamafile language model\n",
    "llm = Llamafile()\n",
    "\n",
    "\n",
    "# Define a function to format documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Pull a predefined RAG (Retrieval-Augmented Generation) prompt template from the hub\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define the question to be asked\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create a question-answering chain using the retriever, RAG prompt, LLM, and output parser\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the question-answering chain with the specified question and print the result\n",
    "print(qa_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Query Analysis System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Getting Started with Multi-Modal LLMs', '2023-12-20 00:00:00'), ('LangServe and LangChain Templates Webinar', '2023-11-02 00:00:00'), ('Getting Started with Multi-Modal LLMs', '2023-12-20 00:00:00'), ('Building a Research Assistant from Scratch', '2023-11-16 00:00:00')]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the specified version and temperature\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# List of YouTube video URLs to process\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
    "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
    "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
    "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
    "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "]\n",
    "\n",
    "# Load the documents from the YouTube videos\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())\n",
    "\n",
    "# Add additional metadata: the year the video was published\n",
    "for doc in docs:\n",
    "    doc.metadata[\"publish_year\"] = int(\n",
    "        datetime.datetime.strptime(\n",
    "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
    "        ).strftime(\"%Y\")\n",
    "    )\n",
    "\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create embeddings for the document chunks\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunked_docs,\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "# Define a Pydantic model for the search\n",
    "class Search(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")\n",
    "\n",
    "\n",
    "# Define the system prompt for converting user questions into database queries\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a structured LLM that outputs a Search object\n",
    "structured_llm = llm.with_structured_output(Search)\n",
    "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm\n",
    "\n",
    "\n",
    "# Define a retrieval function that searches the vector store based on the search criteria\n",
    "def retrieval(search: Search) -> List[Document]:\n",
    "    if search.publish_year is not None:\n",
    "        # Apply a filter for the publish year if specified\n",
    "        _filter = {\"publish_year\": {\"$eq\": search.publish_year}}\n",
    "    else:\n",
    "        _filter = None\n",
    "    # Perform a similarity search on the vector store\n",
    "    return vectorstore.similarity_search(search.query, filter=_filter)\n",
    "\n",
    "\n",
    "# Create a retrieval chain combining query analysis and retrieval\n",
    "retrieval_chain = query_analyzer | retrieval\n",
    "\n",
    "# Invoke the retrieval chain with a query\n",
    "results = retrieval_chain.invoke(\"RAG tutorial published in 2023\")\n",
    "\n",
    "# Print the titles and publish dates of the retrieved documents\n",
    "print([(doc.metadata[\"title\"], doc.metadata[\"publish_date\"]) for doc in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Question/Answering system over SQL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_nfil0RLn0K4yO8hLJXK7xcY5', 'function': {'arguments': '{\"query\":\"alis in chain\"}', 'name': 'search_proper_nouns'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 674, 'total_tokens': 693}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c8c598b-d28d-4186-9d82-7228f1618f76-0', tool_calls=[{'name': 'search_proper_nouns', 'args': {'query': 'alis in chain'}, 'id': 'call_nfil0RLn0K4yO8hLJXK7xcY5'}], usage_metadata={'input_tokens': 674, 'output_tokens': 19, 'total_tokens': 693})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Alice In Chains\\n\\nAisha Duo\\n\\nXis\\n\\nDa Lama Ao Caos\\n\\nA-Sides', name='search_proper_nouns', tool_call_id='call_nfil0RLn0K4yO8hLJXK7xcY5')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Y1jpNuia5BXlZvzreUbsv2uO', 'function': {'arguments': '{\"query\":\"SELECT COUNT(*) AS album_count FROM Album WHERE ArtistId IN (SELECT ArtistId FROM Artist WHERE Name = \\'Alice In Chains\\')\"}', 'name': 'sql_db_query'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 724, 'total_tokens': 764}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-30a2eb63-6d03-498e-b9bf-efefb0a77fae-0', tool_calls=[{'name': 'sql_db_query', 'args': {'query': \"SELECT COUNT(*) AS album_count FROM Album WHERE ArtistId IN (SELECT ArtistId FROM Artist WHERE Name = 'Alice In Chains')\"}, 'id': 'call_Y1jpNuia5BXlZvzreUbsv2uO'}], usage_metadata={'input_tokens': 724, 'output_tokens': 40, 'total_tokens': 764})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='[(1,)]', name='sql_db_query', tool_call_id='call_Y1jpNuia5BXlZvzreUbsv2uO')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Alice In Chains has 1 album.', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 777, 'total_tokens': 786}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ea762f3d-a900-47a8-98da-875bb19851ef-0', usage_metadata={'input_tokens': 777, 'output_tokens': 9, 'total_tokens': 786})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the specified version\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Create a connection to the SQL database\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "\n",
    "# Create tools from a toolkit for interacting with the SQL database using the LLM\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "\n",
    "# Define a function to run a query on the database and process the results\n",
    "def query_as_list(database, sql_query):\n",
    "    # Run the query on the database\n",
    "    res = database.run(sql_query)\n",
    "    # Parse the result into a list of values\n",
    "    res = [el for sub in ast.literal_eval(res) for el in sub if el]\n",
    "    # Remove numeric values and strip whitespace\n",
    "    res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]\n",
    "    # Return unique values as a list\n",
    "    return list(set(res))\n",
    "\n",
    "\n",
    "# Query the list of artist names and album titles from the database\n",
    "artists = query_as_list(db, \"SELECT Name FROM Artist\")\n",
    "albums = query_as_list(db, \"SELECT Title FROM Album\")\n",
    "\n",
    "# Create a FAISS vector store from the combined list of artists and albums, using OpenAI embeddings\n",
    "vector_db = FAISS.from_texts(artists + albums, OpenAIEmbeddings())\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Define a description for the retriever tool\n",
    "description = \"\"\"Use to look up values to filter on. Input is an approximate spelling of the proper noun, output is \\\n",
    "valid proper nouns. Use the noun most similar to the search.\"\"\"\n",
    "\n",
    "# Create a retriever tool with the defined retriever and description\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"search_proper_nouns\",\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "# Define a system message with instructions for the agent\n",
    "system = \"\"\"You are an agent designed to interact with a SQL database.\n",
    "Given an input question, create a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "You have access to tools for interacting with the database.\n",
    "Only use the given tools. Only use the information returned by the tools to construct your final answer.\n",
    "You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\n",
    "You have access to the following tables: {table_names}\n",
    "\n",
    "If you need to filter on a proper noun, you must ALWAYS first look up the filter value using the \"search_proper_nouns\" tool!\n",
    "Do not try to guess at the proper name - use this function to find similar ones.\"\"\".format(\n",
    "    table_names=db.get_usable_table_names()\n",
    ")\n",
    "\n",
    "# Create a system message with the defined instructions\n",
    "system_message = SystemMessage(content=system)\n",
    "\n",
    "# Append the retriever tool to the list of tools\n",
    "tools.append(retriever_tool)\n",
    "\n",
    "# Create an agent with the LLM, tools, and system message\n",
    "agent = create_react_agent(llm, tools, messages_modifier=system_message)\n",
    "\n",
    "# Define the first query\n",
    "query = \"How many albums does alis in chain have?\"\n",
    "\n",
    "# Stream responses for the first query using the agent\n",
    "for s in agent.stream({\"messages\": [HumanMessage(content=query)]}):\n",
    "    # Print each response and a separator\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 employees.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the specified version\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Create a connection to the SQL database\n",
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "\n",
    "# Create an initial SQL query chain with the LLM and the database\n",
    "chain = create_sql_query_chain(llm, db)\n",
    "\n",
    "# Create a tool to execute SQL queries on the database\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "# Create another SQL query chain for writing queries\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "\n",
    "# Combine the write query chain and the execute query tool into a single chain\n",
    "chain = write_query | execute_query\n",
    "\n",
    "# Define a prompt template to generate an answer based on the user question, SQL query, and SQL result\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "# Combine the query writing, execution, and answering into a single chain\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        query=write_query\n",
    "    ).assign(  # Assign the query writing chain\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )  # Assign the result execution chain\n",
    "    | answer_prompt  # Use the answer prompt to format the response\n",
    "    | llm  # Use the LLM to generate the final answer\n",
    "    | StrOutputParser()  # Parse the output as a string\n",
    ")\n",
    "\n",
    "# Invoke the chain with a user question\n",
    "answer = chain.invoke({\"question\": \"How many employees are there\"})\n",
    "\n",
    "# Print the answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Conversational RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KasVENthsc22RFkLShcQO1ro', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 68, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d8f6069c-a1e5-4e4d-bec3-0e64fabff736-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_KasVENthsc22RFkLShcQO1ro'}], usage_metadata={'input_tokens': 68, 'output_tokens': 19, 'total_tokens': 87})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', name='blog_post_retriever', tool_call_id='call_KasVENthsc22RFkLShcQO1ro')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and structured approach to problem-solving. By decomposing tasks, agents can better understand the steps involved and plan ahead effectively. One common method for task decomposition is the Chain of Thought (CoT) technique, which instructs models to \"think step by step\" and decompose hard tasks into smaller components. This technique enhances model performance on complex tasks by utilizing more test-time computation.', response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 590, 'total_tokens': 700}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-76e138bb-0372-47a9-9890-4fde4fbe155a-0', usage_metadata={'input_tokens': 590, 'output_tokens': 110, 'total_tokens': 700})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xHD7lAimugv43jycSvZrbSZ8', 'function': {'arguments': '{\"query\":\"common ways of task decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 723, 'total_tokens': 744}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-78e75628-1c19-4ce8-804a-7b0487f1d419-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'common ways of task decomposition'}, 'id': 'call_xHD7lAimugv43jycSvZrbSZ8'}], usage_metadata={'input_tokens': 723, 'output_tokens': 21, 'total_tokens': 744})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', name='blog_post_retriever', tool_call_id='call_xHD7lAimugv43jycSvZrbSZ8')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='According to the blog post, common ways of task decomposition include:\\n\\n1. Using LLM with simple prompting, such as \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n2. Using task-specific instructions, for example, \"Write a story outline\" for writing a novel.\\n3. Involving human inputs in the task decomposition process.\\n\\nAdditionally, the Tree of Thoughts technique extends the Chain of Thought (CoT) method by exploring multiple reasoning possibilities at each step. It decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier or majority vote.', response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 1339, 'total_tokens': 1491}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-84da0fe7-80b7-47dc-a0ee-3393856f3b3e-0', usage_metadata={'input_tokens': 1339, 'output_tokens': 152, 'total_tokens': 1491})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize an in-memory SQLite saver for storing checkpoints\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Initialize the OpenAI model with the specified version and temperature\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Load, chunk, and index the contents of the blog\n",
    "\n",
    "# Define a web base loader to load the contents of the specified blog URL\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\n",
    "                \"post-content\",\n",
    "                \"post-title\",\n",
    "                \"post-header\",\n",
    "            )  # Specify the classes to parse\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load the documents from the web page\n",
    "docs = loader.load()\n",
    "\n",
    "# Define a text splitter to chunk the documents into smaller pieces\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into chunks\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a Chroma vector store from the document chunks, using OpenAI embeddings\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Build retriever tool\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",  # Name of the retriever tool\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",  # Description of the retriever tool\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "# Create an agent executor with the LLM, tools, and checkpoint saver\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
    "\n",
    "# Configuration dictionary for the session\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# Define the first query\n",
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "# Stream responses for the query using the agent executor\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    # Print each response and a separator\n",
    "    print(s)\n",
    "    print(\"----\")\n",
    "\n",
    "# Define the second query\n",
    "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
    "\n",
    "# Stream responses for the conversational query using the agent executor\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    # Print each response and a separator\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks to make them easier to accomplish. This approach helps agents or models to better understand and interpret the thinking process involved in completing a task.\n",
      "One common way of task decomposition is through the use of techniques like Chain of Thought (CoT), where models are instructed to \"think step by step\" to break down hard tasks. Another approach is to divide the task into smaller subtasks that can be tackled individually. These methods help in enhancing model performance on complex tasks by simplifying the overall process.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the specified version and temperature\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Construct retriever\n",
    "\n",
    "# Define a web base loader to load the contents of the specified blog URL\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\n",
    "                \"post-content\",\n",
    "                \"post-title\",\n",
    "                \"post-header\",\n",
    "            )  # Specify the classes to parse\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load the documents from the web page\n",
    "docs = loader.load()\n",
    "\n",
    "# Define a text splitter to chunk the documents into smaller pieces\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into chunks\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a Chroma vector store from the document chunks, using OpenAI embeddings\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Contextualize question\n",
    "\n",
    "# Define a system prompt for contextualizing the question\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "# Create a chat prompt template for contextualizing the question\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a history-aware retriever using the LLM and the contextualize question prompt\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Answer question\n",
    "\n",
    "# Define a system prompt for answering the question\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a chat prompt template for answering the question\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain to combine retrieved documents and the question-answering task\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval-augmented generation (RAG) chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "# Statefully manage chat history\n",
    "\n",
    "# Initialize a dictionary to store session histories\n",
    "store = {}\n",
    "\n",
    "\n",
    "# Function to retrieve or create a session history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Create a runnable with message history for the RAG chain\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "# Invoke the conversational RAG chain with the first query\n",
    "print(\n",
    "    conversational_rag_chain.invoke(\n",
    "        {\"input\": \"What is Task Decomposition?\"},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },  # Constructs a key \"abc123\" in `store`.\n",
    "    )[\"answer\"]\n",
    ")\n",
    "\n",
    "# Invoke the conversational RAG chain with the second query\n",
    "print(\n",
    "    conversational_rag_chain.invoke(\n",
    "        {\"input\": \"What are common ways of doing it?\"},\n",
    "        config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    "    )[\"answer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Retrieval Augmented Generation (RAG) Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition involves breaking down complex tasks into smaller and simpler steps. This technique, such as Chain of Thought, helps enhance model performance by transforming big tasks into multiple manageable tasks through a step-by-step approach. It allows for a clearer interpretation of the model's thinking process.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the specified version\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Load, chunk, and index the contents of the blog\n",
    "\n",
    "# Define a web base loader to load the contents of the specified blog URL\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\n",
    "                \"post-content\",\n",
    "                \"post-title\",\n",
    "                \"post-header\",\n",
    "            )  # Specify the classes to parse\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Load the documents from the web page\n",
    "docs = loader.load()\n",
    "\n",
    "# Define a text splitter to chunk the documents into smaller pieces\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into chunks\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a Chroma vector store from the document chunks, using OpenAI embeddings\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Pull a predefined prompt template from the hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Function to format the documents into a single string\n",
    "def format_docs(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "\n",
    "# Define the RAG (Retrieval-Augmented Generation) chain with context retriever, question passthrough, prompt, and LLM\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the RAG chain with a question about Task Decomposition and print the response\n",
    "print(rag_chain.invoke(\"What is Task Decomposition?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hello Bob! How can I assist you today regarding San Francisco?', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 90, 'total_tokens': 104}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c2aaf030-867b-48a8-95bb-0bba95ac8913-0', usage_metadata={'input_tokens': 90, 'output_tokens': 14, 'total_tokens': 104})]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5Hz5aZjPY0XmnJBvvihCl0Wj', 'function': {'arguments': '{\"query\":\"San Francisco weather\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 119, 'total_tokens': 139}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-544d5b48-e9ca-42b6-b5a8-c3771c972c67-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'San Francisco weather'}, 'id': 'call_5Hz5aZjPY0XmnJBvvihCl0Wj'}], usage_metadata={'input_tokens': 119, 'output_tokens': 20, 'total_tokens': 139})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='[{\"url\": \"https://www.accuweather.com/en/us/san-francisco/94103/july-weather/347629\", \"content\": \"Get the monthly weather forecast for San Francisco, CA, including daily high/low, historical averages, to help you plan ahead.Missing:  09/07/2024\"}, {\"url\": \"https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=September\", \"content\": \"The temperatures in San Francisco in September are comfortable with low of 57\\\\u00b0F and and high up to 77\\\\u00b0F. There is little to no rain in San Francisco during\\\\u00a0...\"}]', name='tavily_search_results_json', tool_call_id='call_5Hz5aZjPY0XmnJBvvihCl0Wj')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='The weather in San Francisco for September typically ranges from a low of 57°F to a high of 77°F. There is usually little to no rain during this time. If you would like more detailed information, you can visit [AccuWeather](https://www.accuweather.com/en/us/san-francisco/94103/september-weather/347629) or [Weather25](https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=September).', response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 307, 'total_tokens': 418}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-648f201c-9285-43f3-a157-8de9c00278c1-0', usage_metadata={'input_tokens': 307, 'output_tokens': 111, 'total_tokens': 418})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Initialize the search tool with a limit of 2 results per query\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "# Initialize an in-memory SQLite database for saving agent state\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "# Create a reactive agent executor with the model, tools, and memory\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "# Configuration for the agent's execution, including a thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# Execute the agent with a greeting message and print the response chunks\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"hi im bob! and i live in sf\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")\n",
    "\n",
    "# Execute the agent with a conversational memory query and print the response chunks\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather where I live?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vector stores and retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats are independent pets that often enjoy their own space.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create a list of documents, each with content and metadata\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create a Chroma vector store from the documents, using OpenAI embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# Create a retriever from the vector store for similarity-based search\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},  # Retrieve the top 1 similar document\n",
    ")\n",
    "\n",
    "# Initialize the OpenAI model with the specified version\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Define a message template for the chat prompt\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "# Create a chat prompt template from the message\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "# Define the RAG (Retrieval-Augmented Generation) chain with context retriever and question passthrough\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "# Invoke the RAG chain with a question about cats\n",
    "response = rag_chain.invoke(\"tell me about cats\")\n",
    "\n",
    "# Print the response content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Your| name| is| Bob|.||"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI model with the gpt-3.5-turbo model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize a dictionary to store session histories\n",
    "store = {}\n",
    "\n",
    "\n",
    "# Function to retrieve or create a session history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "# Define a chat prompt template with system and placeholder messages\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a trimmer to trim messages to a maximum token count\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "# Define the initial set of messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "]\n",
    "\n",
    "# Create a runnable chain that processes messages\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "# Create a runnable with message history, binding the chain with the session history function\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "# Configuration dictionary for the session\n",
    "config = {\"configurable\": {\"session_id\": \"abc16\"}}\n",
    "\n",
    "# Stream responses by passing messages and language configuration to the runnable with message history\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple LLM Application with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import uvicorn\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "# 0. Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Create prompt template\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "# 2. Create model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 3. Create parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 4. Create chain\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# 5. App definition\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 6. Adding chain route\n",
    "add_routes(\n",
    "    app,\n",
    "    chain,\n",
    "    path=\"/chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
    "remote_chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
